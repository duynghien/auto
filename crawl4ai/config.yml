# ============================================
# Crawl4AI Server Configuration (v0.8.0)
# ============================================
# This file is mounted into the container at /app/config.yml
# Modify settings here and restart the container to apply.
#
# Priority order for LLM config:
#   1. API Request Parameters (highest)
#   2. Provider-specific env vars (OPENAI_TEMPERATURE, etc.)
#   3. Global env vars (LLM_PROVIDER, LLM_TEMPERATURE, etc.)
#   4. This config file (lowest)

# Application Configuration
app:
  title: "Crawl4AI API"
  version: "0.8.0"
  host: "0.0.0.0"
  port: 11235
  reload: false
  workers: 1
  timeout_keep_alive: 300

# Default LLM (fallback when no env vars or request params)
llm:
  provider: "openai/deepseek-v3.2"  # Match .llm.env
  # api_key: sk-...  # Not recommended, use .llm.env instead

# Internal Redis (managed by supervisord inside container)
redis:
  host: "localhost"
  port: 6379
  db: 0
  password: ""
  ssl: false

# Rate Limiting
rate_limiting:
  enabled: true
  default_limit: "1000/minute"
  trusted_proxies: []
  storage_uri: "memory://"    # Use "redis://localhost:6379" for production

# Security (enable for production/public exposure)
# WARNING: For production, enable security and use proper SECRET_KEY:
#   - Set jwt_enabled: true for authentication
#   - Set SECRET_KEY environment variable to a secure random value
#   - Set CRAWL4AI_HOOKS_ENABLED=true only if you need hooks (RCE risk)
security:
  enabled: false
  jwt_enabled: false
  https_redirect: false
  trusted_hosts: ["*"]
  headers:
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
    content_security_policy: "default-src 'self'"
    strict_transport_security: "max-age=63072000; includeSubDomains"

# Crawler Engine
crawler:
  base_config:
    simulate_user: true
  memory_threshold_percent: 95.0
  rate_limiter:
    enabled: true
    base_delay: [1.0, 2.0]       # Min/max delay between requests
  timeouts:
    stream_init: 30.0            # Timeout for stream initialization
    batch_process: 300.0         # Timeout for batch processing
  pool:
    max_pages: 40                # Max concurrent pages (semaphore permits)
    idle_ttl_sec: 300            # Idle page cleanup after 5 minutes
  browser:
    kwargs:
      headless: true
      text_mode: true
    extra_args:
      - "--no-sandbox"
      - "--disable-dev-shm-usage"
      - "--disable-gpu"
      - "--disable-software-rasterizer"
      - "--disable-web-security"
      - "--allow-insecure-localhost"
      - "--ignore-certificate-errors"

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Observability
observability:
  prometheus:
    enabled: true
    endpoint: "/metrics"
  health_check:
    endpoint: "/health"

# Webhook Configuration
webhooks:
  enabled: true
  default_url: null              # Optional: default webhook URL for all jobs
  data_in_payload: false         # Include crawl data in webhook payload
  retry:
    max_attempts: 5
    initial_delay_ms: 1000       # Exponential backoff: 1s, 2s, 4s, 8s, 16s
    max_delay_ms: 32000
    timeout_ms: 30000            # 30s timeout per webhook call
  headers:
    User-Agent: "Crawl4AI-Webhook/1.0"
